{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>...</th>\n",
       "      <th>submitted_month</th>\n",
       "      <th>submitted_year</th>\n",
       "      <th>dairy-free</th>\n",
       "      <th>gluten-free</th>\n",
       "      <th>low-carb</th>\n",
       "      <th>vegan</th>\n",
       "      <th>vegetarian</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>137739</td>\n",
       "      <td>55</td>\n",
       "      <td>47892</td>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>11</td>\n",
       "      <td>['make a choice and proceed with recipe', 'dep...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>137739</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a bit different  breakfast pizza</td>\n",
       "      <td>31490</td>\n",
       "      <td>30</td>\n",
       "      <td>26278</td>\n",
       "      <td>2002-06-17</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>9</td>\n",
       "      <td>['preheat oven to 425 degrees f', 'press dough...</td>\n",
       "      <td>this recipe calls for the crust to be prebaked...</td>\n",
       "      <td>['prepared pizza crust', 'sausage patty', 'egg...</td>\n",
       "      <td>...</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31490</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all in the kitchen  chili</td>\n",
       "      <td>112140</td>\n",
       "      <td>130</td>\n",
       "      <td>196586</td>\n",
       "      <td>2005-02-25</td>\n",
       "      <td>['time-to-make', 'course', 'preparation', 'mai...</td>\n",
       "      <td>6</td>\n",
       "      <td>['brown ground beef in large pot', 'add choppe...</td>\n",
       "      <td>this modified version of 'mom's' chili was a h...</td>\n",
       "      <td>['ground beef', 'yellow onions', 'diced tomato...</td>\n",
       "      <td>...</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112140</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alouette  potatoes</td>\n",
       "      <td>59389</td>\n",
       "      <td>45</td>\n",
       "      <td>68585</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>11</td>\n",
       "      <td>['place potatoes in a large pot of lightly sal...</td>\n",
       "      <td>this is a super easy, great tasting, make ahea...</td>\n",
       "      <td>['spreadable cheese with garlic and herbs', 'n...</td>\n",
       "      <td>...</td>\n",
       "      <td>Apr</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59389</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amish  tomato ketchup  for canning</td>\n",
       "      <td>44061</td>\n",
       "      <td>190</td>\n",
       "      <td>41706</td>\n",
       "      <td>2002-10-25</td>\n",
       "      <td>['weeknight', 'time-to-make', 'course', 'main-...</td>\n",
       "      <td>5</td>\n",
       "      <td>['mix all ingredients&amp; boil for 2 1 / 2 hours ...</td>\n",
       "      <td>my dh's amish mother raised him on this recipe...</td>\n",
       "      <td>['tomato juice', 'apple cider vinegar', 'sugar...</td>\n",
       "      <td>...</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44061</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name      id  minutes contributor_id  \\\n",
       "0  arriba   baked winter squash mexican style  137739       55          47892   \n",
       "1            a bit different  breakfast pizza   31490       30          26278   \n",
       "2                   all in the kitchen  chili  112140      130         196586   \n",
       "3                          alouette  potatoes   59389       45          68585   \n",
       "4          amish  tomato ketchup  for canning   44061      190          41706   \n",
       "\n",
       "   submitted                                               tags  n_steps  \\\n",
       "0 2005-09-16  ['60-minutes-or-less', 'time-to-make', 'course...       11   \n",
       "1 2002-06-17  ['30-minutes-or-less', 'time-to-make', 'course...        9   \n",
       "2 2005-02-25  ['time-to-make', 'course', 'preparation', 'mai...        6   \n",
       "3 2003-04-14  ['60-minutes-or-less', 'time-to-make', 'course...       11   \n",
       "4 2002-10-25  ['weeknight', 'time-to-make', 'course', 'main-...        5   \n",
       "\n",
       "                                               steps  \\\n",
       "0  ['make a choice and proceed with recipe', 'dep...   \n",
       "1  ['preheat oven to 425 degrees f', 'press dough...   \n",
       "2  ['brown ground beef in large pot', 'add choppe...   \n",
       "3  ['place potatoes in a large pot of lightly sal...   \n",
       "4  ['mix all ingredients& boil for 2 1 / 2 hours ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  autumn is my favorite time of year to cook! th...   \n",
       "1  this recipe calls for the crust to be prebaked...   \n",
       "2  this modified version of 'mom's' chili was a h...   \n",
       "3  this is a super easy, great tasting, make ahea...   \n",
       "4  my dh's amish mother raised him on this recipe...   \n",
       "\n",
       "                                         ingredients  ...  submitted_month  \\\n",
       "0  ['winter squash', 'mexican seasoning', 'mixed ...  ...              Sep   \n",
       "1  ['prepared pizza crust', 'sausage patty', 'egg...  ...              Jun   \n",
       "2  ['ground beef', 'yellow onions', 'diced tomato...  ...              Feb   \n",
       "3  ['spreadable cheese with garlic and herbs', 'n...  ...              Apr   \n",
       "4  ['tomato juice', 'apple cider vinegar', 'sugar...  ...              Oct   \n",
       "\n",
       "   submitted_year  dairy-free  gluten-free  low-carb  vegan  vegetarian  \\\n",
       "0            2005           0            0         0      0           1   \n",
       "1            2002           0            0         0      0           0   \n",
       "2            2005           0            0         0      0           0   \n",
       "3            2003           0            0         0      0           0   \n",
       "4            2002           0            0         0      0           1   \n",
       "\n",
       "   recipe_id average_rating  votes  \n",
       "0     137739       5.000000      3  \n",
       "1      31490       4.666667      3  \n",
       "2     112140       4.000000      1  \n",
       "3      59389       4.500000      2  \n",
       "4      44061       5.000000      1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data =pd.read_pickle(\"food.pkl\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>...</th>\n",
       "      <th>submitted_month</th>\n",
       "      <th>submitted_year</th>\n",
       "      <th>dairy-free</th>\n",
       "      <th>gluten-free</th>\n",
       "      <th>low-carb</th>\n",
       "      <th>vegan</th>\n",
       "      <th>vegetarian</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168477</th>\n",
       "      <td>sesame dipping sauce</td>\n",
       "      <td>353233</td>\n",
       "      <td>10</td>\n",
       "      <td>406741</td>\n",
       "      <td>2009-01-31</td>\n",
       "      <td>['lactose', '15-minutes-or-less', 'time-to-mak...</td>\n",
       "      <td>2</td>\n",
       "      <td>['in a small bowl , whisk together soy sauce ,...</td>\n",
       "      <td>this works well for dipping pot stickers in as...</td>\n",
       "      <td>['soy sauce', 'rice vinegar', 'sesame oil', 'a...</td>\n",
       "      <td>...</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>353233</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201337</th>\n",
       "      <td>unattended rib roast</td>\n",
       "      <td>97973</td>\n",
       "      <td>112</td>\n",
       "      <td>152393</td>\n",
       "      <td>2004-08-17</td>\n",
       "      <td>['time-to-make', 'main-ingredient', 'preparati...</td>\n",
       "      <td>6</td>\n",
       "      <td>['at noon , preheat oven to 375 degrees', 'sea...</td>\n",
       "      <td>i found this recipe in the paper and thought i...</td>\n",
       "      <td>['standing rib roast', 'salt and pepper']</td>\n",
       "      <td>...</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>97973</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173114</th>\n",
       "      <td>slow cooker mashed potatoes</td>\n",
       "      <td>265017</td>\n",
       "      <td>135</td>\n",
       "      <td>290107</td>\n",
       "      <td>2007-11-12</td>\n",
       "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
       "      <td>6</td>\n",
       "      <td>['in a mixing bowl , combine cream cheese , so...</td>\n",
       "      <td>this recipe is from taste of home magazine. it...</td>\n",
       "      <td>['cream cheese', 'sour cream', 'butter', 'drie...</td>\n",
       "      <td>...</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>265017</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29931</th>\n",
       "      <td>butterscotch toffee cookies</td>\n",
       "      <td>288112</td>\n",
       "      <td>25</td>\n",
       "      <td>668077</td>\n",
       "      <td>2008-02-23</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>8</td>\n",
       "      <td>['mix shortening and both sugars with electric...</td>\n",
       "      <td>in preparation for a bake sale, i changed a re...</td>\n",
       "      <td>['shortening', 'sugar', 'brown sugar', 'egg', ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>288112</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9300</th>\n",
       "      <td>au gratin hash browns casserole</td>\n",
       "      <td>94740</td>\n",
       "      <td>70</td>\n",
       "      <td>126418</td>\n",
       "      <td>2004-06-30</td>\n",
       "      <td>['time-to-make', 'course', 'preparation', 'cas...</td>\n",
       "      <td>6</td>\n",
       "      <td>['oven@ 350', 'in a large bowl combine first 5...</td>\n",
       "      <td>i think i found this recipe in my \"goody bag\" ...</td>\n",
       "      <td>['cream of chicken soup', 'sour cream', 'marga...</td>\n",
       "      <td>...</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94740</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name      id  minutes contributor_id  \\\n",
       "168477             sesame dipping sauce  353233       10         406741   \n",
       "201337             unattended rib roast   97973      112         152393   \n",
       "173114      slow cooker mashed potatoes  265017      135         290107   \n",
       "29931       butterscotch toffee cookies  288112       25         668077   \n",
       "9300    au gratin hash browns casserole   94740       70         126418   \n",
       "\n",
       "        submitted                                               tags  n_steps  \\\n",
       "168477 2009-01-31  ['lactose', '15-minutes-or-less', 'time-to-mak...        2   \n",
       "201337 2004-08-17  ['time-to-make', 'main-ingredient', 'preparati...        6   \n",
       "173114 2007-11-12  ['time-to-make', 'course', 'main-ingredient', ...        6   \n",
       "29931  2008-02-23  ['30-minutes-or-less', 'time-to-make', 'course...        8   \n",
       "9300   2004-06-30  ['time-to-make', 'course', 'preparation', 'cas...        6   \n",
       "\n",
       "                                                    steps  \\\n",
       "168477  ['in a small bowl , whisk together soy sauce ,...   \n",
       "201337  ['at noon , preheat oven to 375 degrees', 'sea...   \n",
       "173114  ['in a mixing bowl , combine cream cheese , so...   \n",
       "29931   ['mix shortening and both sugars with electric...   \n",
       "9300    ['oven@ 350', 'in a large bowl combine first 5...   \n",
       "\n",
       "                                              description  \\\n",
       "168477  this works well for dipping pot stickers in as...   \n",
       "201337  i found this recipe in the paper and thought i...   \n",
       "173114  this recipe is from taste of home magazine. it...   \n",
       "29931   in preparation for a bake sale, i changed a re...   \n",
       "9300    i think i found this recipe in my \"goody bag\" ...   \n",
       "\n",
       "                                              ingredients  ...  \\\n",
       "168477  ['soy sauce', 'rice vinegar', 'sesame oil', 'a...  ...   \n",
       "201337          ['standing rib roast', 'salt and pepper']  ...   \n",
       "173114  ['cream cheese', 'sour cream', 'butter', 'drie...  ...   \n",
       "29931   ['shortening', 'sugar', 'brown sugar', 'egg', ...  ...   \n",
       "9300    ['cream of chicken soup', 'sour cream', 'marga...  ...   \n",
       "\n",
       "        submitted_month  submitted_year  dairy-free  gluten-free  low-carb  \\\n",
       "168477              Jan            2009           0            0         0   \n",
       "201337              Aug            2004           0            0         1   \n",
       "173114              Nov            2007           0            0         0   \n",
       "29931               Feb            2008           0            0         0   \n",
       "9300                Jun            2004           0            0         0   \n",
       "\n",
       "        vegan  vegetarian  recipe_id average_rating  votes  \n",
       "168477      1           1     353233            5.0      2  \n",
       "201337      0           0      97973            5.0      1  \n",
       "173114      0           0     265017            5.0      2  \n",
       "29931       0           0     288112            4.0      1  \n",
       "9300        0           0      94740            5.0      1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sampled Dataset: (21226, 28)\n"
     ]
    }
   ],
   "source": [
    "# Sample 10% of the dataset\n",
    "sampled_data = data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# View sample\n",
    "display(sampled_data.head())\n",
    "\n",
    "# Print the shape of the sampled dataset\n",
    "print(\"Shape of Sampled Dataset:\", sampled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kelly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for tokenizer\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def recipe_tokenizer(sentence):\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "\n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for word embedding using Word2Vec\n",
    "def word_embedding(sampled_data, column):\n",
    "    # Tokenize the text data\n",
    "    tokenized_data = sampled_data[column].apply(recipe_tokenizer)\n",
    "\n",
    "    # Train a Word2Vec model\n",
    "    model = Word2Vec(tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Create word embeddings for each word in the vocabulary\n",
    "    embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "\n",
    "    # Save the trained Word2Vec model\n",
    "    model.save('word2vec_model.bin')\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pre-compute and store the combined embeddings\n",
    "def precompute_embeddings(sampled_data):\n",
    "    # Step 1: Process 'ingredients' using word2vec and create word embeddings\n",
    "    embeddings = word_embedding(sampled_data, 'ingredients')\n",
    "\n",
    "    # Step 2: Concatenate relevant columns (excluding 'ingredients')\n",
    "    sampled_data['text_data'] = sampled_data[['name', 'tags', 'description']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    # Step 3: Preprocess the text data (example: lowercase conversion)\n",
    "    sampled_data['text_data'] = sampled_data['text_data'].str.lower()\n",
    "\n",
    "    # Step 4: Vectorize the text data (excluding 'ingredients') using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(min_df=5,\n",
    "                                 tokenizer=recipe_tokenizer)\n",
    "    vectorized_data = vectorizer.fit_transform(sampled_data['text_data'])\n",
    "\n",
    "    # Step 5: Retrieve the word embeddings for 'ingredients'\n",
    "    ingredient_embeddings = [np.mean([embeddings[word] for word in recipe_tokenizer(ingredients) if word in embeddings]\n",
    "                                      or [np.zeros(100)], axis=0) for ingredients in sampled_data['ingredients']]\n",
    "\n",
    "    # Step 6: Combine the vectorized data and ingredient embeddings\n",
    "    combined_embeddings = np.concatenate([vectorized_data.toarray(), np.array(ingredient_embeddings)], axis=1)\n",
    "    \n",
    "    # Step 7: Store combined embeddings in pkl file\n",
    "    with open('combined_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(combined_embeddings, f)\n",
    "    \n",
    "    # Step 8: Store the trained TF-IDF vectorizer model in a separate pkl file\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    \n",
    "    # Step 9: Done!  \n",
    "    print(\"Text data and TF-IDF vectorizer model stored in pkl files!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data and TF-IDF vectorizer model stored in pkl files!\n"
     ]
    }
   ],
   "source": [
    "# store vectorized data and the trained TF-IDF vectorizer model from sampled data\n",
    "precompute_embeddings(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the Word2Vec model\n",
    "def load_word2vec_model():\n",
    "    return Word2Vec.load('word2vec_model.bin')\n",
    "\n",
    "# Function to load the combined embeddings and TF-IDF vectorizer model\n",
    "def load_embeddings_and_vectorizer():\n",
    "    with open('combined_embeddings.pkl', 'rb') as f:\n",
    "        combined_embeddings = pickle.load(f)\n",
    "    with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    return combined_embeddings, vectorizer\n",
    "\n",
    "# Function for finding recipes\n",
    "def find_similar_recipes(sampled_data, user_input, num_similar=5):\n",
    "    # Load the Word2Vec model\n",
    "    word2vec_model = load_word2vec_model()\n",
    "\n",
    "    try:\n",
    "        combined_embeddings, vectorizer = load_embeddings_and_vectorizer()\n",
    "    except FileNotFoundError:\n",
    "        precompute_embeddings(sampled_data)\n",
    "        combined_embeddings, vectorizer = load_embeddings_and_vectorizer()\n",
    "\n",
    "    # Process user input\n",
    "    # Create a DataFrame for user input\n",
    "    user_data = pd.DataFrame({'text_data': [user_input]})\n",
    "    user_data['text_data'] = user_data['text_data'].str.lower()\n",
    "\n",
    "    # Tokenize and create word embeddings for user input\n",
    "    user_tokens = recipe_tokenizer(user_data['text_data'][0])\n",
    "    user_doc_embeddings = []\n",
    "    for token in user_tokens:\n",
    "        try:\n",
    "            embedding = word2vec_model.wv.get_vector(token)\n",
    "            user_doc_embeddings.append(embedding)\n",
    "        except KeyError:\n",
    "            # Token not found in Word2Vec model, ignore it\n",
    "            pass\n",
    "\n",
    "    if not user_doc_embeddings:\n",
    "        # If none of the user's input tokens are in the Word2Vec model, create a zero vector\n",
    "        user_doc_embedding = np.zeros(word2vec_model.vector_size)\n",
    "    else:\n",
    "        user_doc_embedding = np.mean(user_doc_embeddings, axis=0)\n",
    "\n",
    "    # Ensure the number of features in user_vectorized_data matches with combined_embeddings\n",
    "    num_missing_features = combined_embeddings.shape[1] - user_doc_embedding.shape[0]\n",
    "    if num_missing_features > 0:\n",
    "        # Add zero columns to user_doc_embedding to match the number of features\n",
    "        user_doc_embedding = np.pad(user_doc_embedding, (0, num_missing_features))\n",
    "\n",
    "    # Combine the vectorized data and user input embeddings\n",
    "    user_vectorized_data = vectorizer.transform(user_data['text_data']).toarray()\n",
    "    vector_size_match = min(user_vectorized_data.shape[1], word2vec_model.vector_size)\n",
    "    user_doc_embedding = user_doc_embedding[:vector_size_match]\n",
    "    combined_user_embeddings = np.concatenate([user_vectorized_data, user_doc_embedding.reshape(1, -1)], axis=1)\n",
    "\n",
    "    # Slice the matrix to keep only the relevant features\n",
    "    combined_user_embeddings = combined_user_embeddings[:, :combined_embeddings.shape[1]]\n",
    "\n",
    "    # Compute cosine similarity with user input\n",
    "    cosine_sim_matrix = cosine_similarity(combined_user_embeddings, combined_embeddings)\n",
    "\n",
    "    # Retrieve similar recipe indices\n",
    "    similar_recipes = cosine_sim_matrix[0].argsort()[::-1][:num_similar]\n",
    "\n",
    "    # Get similar recipe names from sampled_data\n",
    "    similar_recipe_names = sampled_data.iloc[similar_recipes]['name'].tolist()\n",
    "\n",
    "    return similar_recipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acadia s baked eggplant',\n",
       " 'janae s spaghetti',\n",
       " 'cheesy rice  mushrooms and peas',\n",
       " 'slow cooker spaghetti   meatballs',\n",
       " 'ramen with peas and parmesan']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "find_similar_recipes(sampled_data, \"japanese noodles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for finding recipes\n",
    "def find_similar_recipes(food_df, recipe_index, num_similar=5):\n",
    "    # Step 1: Process 'ingredients' using word2vec and create word embeddings\n",
    "    embeddings = word_embedding(food_df, 'ingredients')\n",
    "\n",
    "    # Step 2: Concatenate relevant columns (excluding 'ingredients')\n",
    "    food_df['text_data'] = food_df[['name', 'tags', 'description']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    # Step 3: Preprocess the text data (example: lowercase conversion)\n",
    "    food_df['text_data'] = food_df['text_data'].str.lower()\n",
    "\n",
    "    # Step 4: Vectorize the text data (excluding 'ingredients') using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(min_df=5,\n",
    "                                 tokenizer=recipe_tokenizer)\n",
    "    vectorized_data = vectorizer.fit_transform(food_df['text_data'])\n",
    "\n",
    "    # Step 5: Retrieve the word embeddings for 'ingredients'\n",
    "    ingredient_embeddings = [np.mean([embeddings[word] for word in recipe_tokenizer(ingredients) if word in embeddings]\n",
    "                                      or [np.zeros(100)], axis=0) for ingredients in food_df['ingredients']]\n",
    "\n",
    "    # Step 6: Combine the vectorized data and ingredient embeddings\n",
    "    combined_embeddings = np.concatenate([vectorized_data.toarray(), np.array(ingredient_embeddings)], axis=1)\n",
    "\n",
    "    # Step 7: Compute cosine similarity\n",
    "    cosine_sim_matrix = cosine_similarity(combined_embeddings)\n",
    "\n",
    "    # Step 8: Retrieve similar recipes\n",
    "    similar_recipes = cosine_sim_matrix[recipe_index].argsort()[::-1][1:num_similar + 1]  # Exclude the recipe itself\n",
    "\n",
    "    # Get similar recipe names\n",
    "    similar_recipe_names = food_df.loc[similar_recipes, 'name'].tolist()\n",
    "\n",
    "    return similar_recipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the batch_processing function to print the name of the recipe being compared\n",
    "def batch_processing(data_df, batch_size):\n",
    "    num_rows = data_df.shape[0]\n",
    "    num_batches = (num_rows // batch_size) + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_rows)\n",
    "\n",
    "        batch_data = data_df.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Perform your desired operations on the batch_data\n",
    "        # For example, you can call your find_similar_recipes function here\n",
    "        for recipe_index in range(batch_data.shape[0]):\n",
    "            recipe_name = batch_data.iloc[recipe_index]['name']\n",
    "            similar_recipes = find_similar_recipes(batch_data, recipe_index, num_similar=5)\n",
    "            print(f\"Batch {i+1}, Recipe Name: {recipe_name}, Similar Recipes:\")\n",
    "            for j, recipe in enumerate(similar_recipes):\n",
    "                print(f\"{j+1}. {recipe}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recipe Name: arriba   baked winter squash mexican style, Similar Recipes:\n",
      "1. granny squash   creamed squash\n",
      "2. acorn squash roasted with applesauce\n",
      "3. acorn squash stuffed with lamb   curry\n",
      "4. acorn squash soup\n",
      "5. acorn or butternut squash risotto\n",
      "Batch 1, Recipe Name: a bit different  breakfast pizza, Similar Recipes:\n",
      "1. a different  pizza\n",
      "2. alfredo deep dish pizza\n",
      "3. an aussie vegemite pizza\n",
      "4. alabama egg and sausage souffle\n",
      "5. amazing quiche\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Call the batch processing function\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m batch_processing(food_df, batch_size)\n",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m, in \u001b[0;36mbatch_processing\u001b[1;34m(data_df, batch_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m recipe_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     15\u001b[0m     recipe_name \u001b[39m=\u001b[39m batch_data\u001b[39m.\u001b[39miloc[recipe_index][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 16\u001b[0m     similar_recipes \u001b[39m=\u001b[39m find_similar_recipes(batch_data, recipe_index, num_similar\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Recipe Name: \u001b[39m\u001b[39m{\u001b[39;00mrecipe_name\u001b[39m}\u001b[39;00m\u001b[39m, Similar Recipes:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m j, recipe \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(similar_recipes):\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36mfind_similar_recipes\u001b[1;34m(food_df, recipe_index, num_similar)\u001b[0m\n\u001b[0;32m     15\u001b[0m vectorized_data \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(food_df[\u001b[39m'\u001b[39m\u001b[39mtext_data\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[39m# Step 5: Retrieve the word embeddings for 'ingredients'\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m ingredient_embeddings \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mmean([embeddings[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m recipe_tokenizer(ingredients) \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m embeddings]\n\u001b[0;32m     19\u001b[0m                                   \u001b[39mor\u001b[39;00m [np\u001b[39m.\u001b[39mzeros(\u001b[39m100\u001b[39m)], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m ingredients \u001b[39min\u001b[39;00m food_df[\u001b[39m'\u001b[39m\u001b[39mingredients\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     21\u001b[0m \u001b[39m# Step 6: Combine the vectorized data and ingredient embeddings\u001b[39;00m\n\u001b[0;32m     22\u001b[0m combined_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([vectorized_data\u001b[39m.\u001b[39mtoarray(), np\u001b[39m.\u001b[39marray(ingredient_embeddings)], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m vectorized_data \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(food_df[\u001b[39m'\u001b[39m\u001b[39mtext_data\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[39m# Step 5: Retrieve the word embeddings for 'ingredients'\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m ingredient_embeddings \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39;49mmean([embeddings[word] \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m recipe_tokenizer(ingredients) \u001b[39mif\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m embeddings]\n\u001b[0;32m     19\u001b[0m                                   \u001b[39mor\u001b[39;49;00m [np\u001b[39m.\u001b[39;49mzeros(\u001b[39m100\u001b[39;49m)], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;00m ingredients \u001b[39min\u001b[39;00m food_df[\u001b[39m'\u001b[39m\u001b[39mingredients\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     21\u001b[0m \u001b[39m# Step 6: Combine the vectorized data and ingredient embeddings\u001b[39;00m\n\u001b[0;32m     22\u001b[0m combined_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([vectorized_data\u001b[39m.\u001b[39mtoarray(), np\u001b[39m.\u001b[39marray(ingredient_embeddings)], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kelly\\anaconda3\\envs\\capstone\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3461\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3462\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 3464\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   3465\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kelly\\anaconda3\\envs\\capstone\\lib\\site-packages\\numpy\\core\\_methods.py:165\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mean\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 165\u001b[0m     arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[0;32m    167\u001b[0m     is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39mkeepdims, where\u001b[39m=\u001b[39mwhere)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 5000\n",
    "\n",
    "# Call the batch processing function\n",
    "batch_processing(food_df, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
